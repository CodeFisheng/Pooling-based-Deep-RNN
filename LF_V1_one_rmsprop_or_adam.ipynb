{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning (RNN) Demo for Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: setting all global parameters -- sec 2 network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time1 = time.time() # set up counter to record run time\n",
    "data_dir = './data/' # directory contains input data\n",
    "num_epoches = 6000 # training epoches for each customer samples\n",
    "n_steps = 48 # input size\n",
    "test_batch_size = 70*48 # days of a batch\n",
    "train_batch_size = 10*48\n",
    "feature_size = 1 # same time of a week\n",
    "n_hidden = 20 # input size\n",
    "num_layers = 3\n",
    "n_output = 1\n",
    "Rs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 4: define data generating function code. \n",
    "which generate a batch of batch-size large sequence data. the data is feature_size dims width and is a time series of float32 of steps steps. inputs and outputs are:\n",
    "\n",
    "inputs:\n",
    "----n_batch: number of samples in a batch\n",
    "----steps: the sequence length of a sample data\n",
    "----feature_size: dimensions of a single time step data frame\n",
    "\n",
    "outputs:\n",
    "----X inputs, shape(n_batch,steps,feature_size)\n",
    "----Y outputs should be, shape(n_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_gen(totaltraindays,x_data,y_data,steps = 48, n_batch = train_batch_size):\n",
    "    X = np.zeros((n_batch,steps,feature_size))\n",
    "    Y = np.zeros((n_batch,feature_size))\n",
    "    rang = range(totaltraindays) # test day sample range\n",
    "    train_days_list = rd.sample(rang,n_batch) # pick unduplicated n indexes as examples\n",
    "    #print totaltraindays\n",
    "    tmpX = [x_data[i,0-steps:] for i in train_days_list]\n",
    "    tmpY = [y_data[i,:] for i in train_days_list]\n",
    "    X = np.array(tmpX).reshape(n_batch,steps,feature_size)\n",
    "    Y = np.array(tmpY).reshape(n_batch,feature_size)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data_gen(x_data,y_data,steps = 48, n_batch = test_batch_size):\n",
    "    X = np.zeros((n_batch,steps,feature_size))\n",
    "    Y = np.zeros((n_batch,feature_size))\n",
    "    #print x_data[:,0-steps:].shape,y_data.shape\n",
    "    #print n_batch, steps\n",
    "    X = x_data[:,0-steps:].reshape(n_batch,steps,feature_size)\n",
    "    Y = y_data.reshape(n_batch,feature_size)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: construct RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create placeholder for x and y\n",
    "x = tf.placeholder(\"float\",[None,n_steps,feature_size])\n",
    "istate = tf.placeholder(\"float\",[None,num_layers*2*n_hidden])\n",
    "y = tf.placeholder(\"float\",[None,n_output])\n",
    "\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([feature_size, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_output]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(_X, _istate, _weights, _biases):\n",
    "\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, feature_size]) # (n_steps*batch_size, n_input)\n",
    "    # Linear activation\n",
    "    _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    stacked_lstm_cell = rnn_cell.MultiRNNCell([lstm_cell]*num_layers)\n",
    "    \n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(stacked_lstm_cell, _X, initial_state=_istate)\n",
    "\n",
    "    # Linear activation\n",
    "    # Get inner loop last output\n",
    "    return tf.matmul(outputs[-1], _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = RNN(x, istate, weights, biases)\n",
    "\n",
    "#cost function \n",
    "cost = tf.reduce_mean(tf.pow(pred-y,2)) # cost function of this batch of data\n",
    "#compute parameter updates\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer2 = tf.train.RMSPropOptimizer(0.005, 0.3).minimize(cost2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "-0.138687726901\n",
      "Iter 10\n",
      "-0.188801437343\n",
      "Iter 20\n",
      "0.101280484665\n",
      "Iter 30\n",
      "0.307724778668\n",
      "Iter 40\n",
      "0.34969742069\n",
      "Iter 50\n",
      "0.372270283434\n",
      "Iter 60\n",
      "0.398813142633\n",
      "Iter 70\n",
      "0.427194887349\n",
      "Iter 80\n",
      "0.464317910429\n",
      "Iter 90\n",
      "0.483649893331\n",
      "Iter 100\n",
      "0.492447626467\n",
      "Iter 110\n",
      "0.517304714437\n",
      "Iter 120\n",
      "0.51851458213\n",
      "Iter 130\n",
      "0.535000436943\n",
      "Iter 140\n",
      "0.540230872684\n",
      "Iter 150\n",
      "0.553548726132\n",
      "Iter 160\n",
      "0.55862332999\n",
      "Iter 170\n",
      "0.569806541069\n",
      "Iter 180\n",
      "0.5797235111\n",
      "Iter 190\n",
      "0.590271695185\n",
      "Iter 200\n",
      "0.596979721647\n",
      "Iter 210\n",
      "0.608585321729\n",
      "Iter 220\n",
      "0.616817147084\n",
      "Iter 230\n",
      "0.623433112179\n",
      "Iter 240\n",
      "0.629480070188\n",
      "Iter 250\n",
      "0.635668102312\n",
      "Iter 260\n",
      "0.642846785161\n",
      "Iter 270\n",
      "0.648411612958\n",
      "Iter 280\n",
      "0.649625317574\n",
      "Iter 290\n",
      "0.655603001333\n",
      "Iter 300\n",
      "0.661715288225\n",
      "Iter 310\n",
      "0.665712094858\n",
      "Iter 320\n",
      "0.670035076581\n",
      "Iter 330\n",
      "0.674431055756\n",
      "Iter 340\n",
      "0.67710725215\n",
      "Iter 350\n",
      "0.680937484053\n",
      "Iter 360\n",
      "0.680534718534\n",
      "Iter 370\n",
      "0.683153581878\n",
      "Iter 380\n",
      "0.68160777482\n",
      "Iter 390\n",
      "0.685113629297\n",
      "Iter 400\n",
      "0.685869658\n",
      "Iter 410\n",
      "0.686493742048\n",
      "Iter 420\n",
      "0.68895807435\n",
      "Iter 430\n",
      "0.687034125552\n",
      "Iter 440\n",
      "0.689474947679\n",
      "Iter 450\n",
      "0.688863241773\n",
      "Iter 460\n",
      "0.689973841387\n",
      "Iter 470\n",
      "0.691522238054\n",
      "Iter 480\n",
      "0.689774853212\n",
      "Iter 490\n",
      "0.692839215106\n",
      "Iter 500\n",
      "0.690972149642\n",
      "Iter 510\n",
      "0.693608278998\n",
      "Iter 520\n",
      "0.692048506355\n",
      "Iter 530\n",
      "0.694070118226\n",
      "Iter 540\n",
      "0.69286135465\n",
      "Iter 550\n",
      "0.685520838068\n",
      "Iter 560\n",
      "0.691283451881\n",
      "Iter 570\n",
      "0.692410565165\n",
      "Iter 580\n",
      "0.6938889936\n",
      "Iter 590\n",
      "0.695082773048\n",
      "Iter 600\n",
      "0.698779795879\n",
      "Iter 610\n",
      "0.699910722803\n",
      "Iter 620\n",
      "0.700280190954\n",
      "Iter 630\n",
      "0.69876035374\n",
      "Iter 640\n",
      "0.699254630489\n",
      "Iter 650\n",
      "0.69855970747\n",
      "Iter 660\n",
      "0.698333910397\n",
      "Iter 670\n",
      "0.69973373616\n",
      "Iter 680\n",
      "0.69921378678\n",
      "Iter 690\n",
      "0.700668409818\n",
      "Iter 700\n",
      "0.697344318639\n",
      "Iter 710\n",
      "0.699283597904\n",
      "Iter 720\n",
      "0.699756879779\n",
      "Iter 730\n",
      "0.697480921746\n",
      "Iter 740\n",
      "0.698834225169\n",
      "Iter 750\n",
      "0.695305087011\n",
      "Iter 760\n",
      "0.701017568724\n",
      "Iter 770\n",
      "0.699724195115\n",
      "Iter 780\n",
      "0.700201768571\n",
      "Iter 790\n",
      "0.697852311975\n",
      "Iter 800\n",
      "0.699809429179\n",
      "Iter 810\n",
      "0.695334000583\n",
      "Iter 820\n",
      "0.686538891012\n",
      "Iter 830\n",
      "0.697745578026\n",
      "Iter 840\n",
      "0.698087839853\n",
      "Iter 850\n",
      "0.699943814717\n",
      "Iter 860\n",
      "0.700274205179\n",
      "Iter 870\n",
      "0.700787732591\n",
      "Iter 880\n",
      "0.699278126439\n",
      "Iter 890\n",
      "0.700168956436\n",
      "Iter 900\n",
      "0.695966175107\n",
      "Iter 910\n",
      "0.699451217012\n",
      "Iter 920\n",
      "0.700309932864\n",
      "Iter 930\n",
      "0.699268137097\n",
      "Iter 940\n",
      "0.700305524542\n",
      "Iter 950\n",
      "0.701845737181\n",
      "Iter 960\n",
      "0.698697030279\n",
      "Iter 970\n",
      "0.70053084379\n",
      "Iter 980\n",
      "0.701107369251\n",
      "Iter 990\n",
      "0.702577352673\n",
      "Iter 1000\n",
      "0.700642573698\n",
      "Iter 1010\n",
      "0.700687442981\n",
      "Iter 1020\n",
      "0.701042039943\n",
      "Iter 1030\n",
      "0.701115693585\n",
      "Iter 1040\n",
      "0.702858768703\n",
      "Iter 1050\n",
      "0.694086435405\n",
      "Iter 1060\n",
      "0.701591659138\n",
      "Iter 1070\n",
      "0.700379017453\n",
      "Iter 1080\n",
      "0.702293600217\n",
      "Iter 1090\n",
      "0.70018098588\n",
      "Iter 1100\n",
      "0.701599807058\n",
      "Iter 1110\n",
      "0.702726314207\n",
      "Iter 1120\n",
      "0.698991552871\n",
      "Iter 1130\n",
      "0.699766880562\n",
      "Iter 1140\n",
      "0.704139225769\n",
      "Iter 1150\n",
      "0.70407530976\n",
      "Iter 1160\n",
      "0.703130593088\n",
      "Iter 1170\n",
      "0.701285008149\n",
      "Iter 1180\n",
      "0.704422346502\n",
      "Iter 1190\n",
      "0.703248968845\n",
      "Iter 1200\n",
      "0.703292954718\n",
      "Iter 1210\n",
      "0.702841879872\n",
      "Iter 1220\n",
      "0.703848480752\n",
      "Iter 1230\n",
      "0.703037352583\n",
      "Iter 1240\n",
      "0.700613486612\n",
      "Iter 1250\n",
      "0.702971625425\n",
      "Iter 1260\n",
      "0.703973725689\n",
      "Iter 1270\n",
      "0.701979220881\n",
      "Iter 1280\n",
      "0.704269155413\n",
      "Iter 1290\n",
      "0.702724056763\n",
      "Iter 1300\n",
      "0.703338624724\n",
      "Iter 1310\n",
      "0.703075925432\n",
      "Iter 1320\n",
      "0.704583139883\n",
      "Iter 1330\n",
      "0.704655434591\n",
      "Iter 1340\n",
      "0.704355849125\n",
      "Iter 1350\n",
      "0.703037894162\n",
      "Iter 1360\n",
      "0.704506086159\n",
      "Iter 1370\n",
      "0.700838571134\n",
      "Iter 1380\n",
      "0.703700944995\n",
      "Iter 1390\n",
      "0.701881274311\n",
      "Iter 1400\n",
      "0.701561330302\n",
      "Iter 1410\n",
      "0.703404330834\n",
      "Iter 1420\n",
      "0.702275112379\n",
      "Iter 1430\n",
      "0.701397573459\n",
      "Iter 1440\n",
      "0.701431077785\n",
      "Iter 1450\n",
      "0.698859222242\n",
      "Iter 1460\n",
      "0.703151969103\n",
      "Iter 1470\n",
      "0.703446145087\n",
      "Iter 1480\n",
      "0.704543981636\n",
      "Iter 1490\n",
      "0.70196529642\n",
      "Iter 1500\n",
      "0.704050807527\n",
      "Iter 1510\n",
      "0.704810418883\n",
      "Iter 1520\n",
      "0.702791841169\n",
      "Iter 1530\n",
      "0.704968637526\n",
      "Iter 1540\n",
      "0.704168826372\n",
      "Iter 1550\n",
      "0.705978783957\n",
      "Iter 1560\n",
      "0.704993453456\n",
      "Iter 1570\n",
      "0.702995526894\n",
      "Iter 1580\n",
      "0.704646185861\n",
      "Iter 1590\n",
      "0.704205853729\n",
      "Iter 1600\n",
      "0.702652348401\n",
      "Iter 1610\n",
      "0.704968902404\n",
      "Iter 1620\n",
      "0.704980361173\n",
      "Iter 1630\n",
      "0.703484035672\n",
      "Iter 1640\n",
      "0.7029725204\n",
      "Iter 1650\n",
      "0.705077900613\n",
      "Iter 1660\n",
      "0.705134533355\n",
      "Iter 1670\n",
      "0.699942316821\n",
      "Iter 1680\n",
      "0.706178762837\n",
      "Iter 1690\n",
      "0.705457381251\n",
      "Iter 1700\n",
      "0.702091611298\n",
      "Iter 1710\n",
      "0.704342758252\n",
      "Iter 1720\n",
      "0.704013991707\n",
      "Iter 1730\n",
      "0.704090564821\n",
      "Iter 1740\n",
      "0.702222789706\n",
      "Iter 1750\n",
      "0.703265101126\n",
      "Iter 1760\n",
      "0.703259474821\n",
      "Iter 1770\n",
      "0.703922979791\n",
      "Iter 1780\n",
      "0.704943553501\n",
      "Iter 1790\n",
      "0.705435879617\n",
      "Iter 1800\n",
      "0.705647153423\n",
      "Iter 1810\n",
      "0.706520648871\n",
      "Iter 1820\n",
      "0.704369174026\n",
      "Iter 1830\n",
      "0.706834190812\n",
      "Iter 1840\n",
      "0.705443531039\n",
      "Iter 1850\n",
      "0.701287897918\n",
      "Iter 1860\n",
      "0.704303972213\n",
      "Iter 1870\n",
      "0.702361035786\n",
      "Iter 1880\n",
      "0.704012405852\n",
      "Iter 1890\n",
      "0.704576893196\n",
      "Iter 1900\n",
      "0.704849317787\n",
      "Iter 1910\n",
      "0.70234316137\n",
      "Iter 1920\n",
      "0.702997434427\n",
      "Iter 1930\n",
      "0.704845126517\n",
      "Iter 1940\n",
      "0.705041409045\n",
      "Iter 1950\n",
      "0.705656187401\n",
      "Iter 1960\n",
      "0.702517256865\n",
      "Iter 1970\n",
      "0.70443407383\n",
      "Iter 1980\n",
      "0.703845016579\n",
      "Iter 1990\n",
      "0.702275533687\n",
      "Iter 2000\n",
      "0.704636649354\n",
      "Iter 2010\n",
      "0.704765716711\n",
      "Iter 2020\n",
      "0.704506640719\n",
      "Iter 2030\n",
      "0.701645651405\n",
      "Iter 2040\n",
      "0.706207104947\n",
      "Iter 2050\n",
      "0.706366399189\n",
      "Iter 2060\n",
      "0.705373309968\n",
      "Iter 2070\n",
      "0.700883968314\n",
      "Iter 2080\n",
      "0.707550241257\n",
      "Iter 2090\n",
      "0.704659582232\n",
      "Iter 2100\n",
      "0.704074315188\n",
      "Iter 2110\n",
      "0.706541408243\n",
      "Iter 2120\n",
      "0.706050373346\n",
      "Iter 2130\n",
      "0.702967472837\n",
      "Iter 2140\n",
      "0.709729884595\n",
      "Iter 2150\n",
      "0.706357765625\n",
      "Iter 2160\n",
      "0.708140021376\n",
      "Iter 2170\n",
      "0.706914773426\n",
      "Iter 2180\n",
      "0.70504945754\n",
      "Iter 2190\n",
      "0.707498042521\n",
      "Iter 2200\n",
      "0.70593120326\n",
      "Iter 2210\n",
      "0.706470323411\n",
      "Iter 2220\n",
      "0.706269480459\n",
      "Iter 2230\n",
      "0.707776425573\n",
      "Iter 2240\n",
      "0.703381774441\n",
      "Iter 2250\n",
      "0.704527506249\n",
      "Iter 2260\n",
      "0.70708516197\n",
      "Iter 2270\n",
      "0.709761511327\n",
      "Iter 2280\n",
      "0.70804006143\n",
      "Iter 2290\n",
      "0.708255829695\n",
      "Iter 2300\n",
      "0.70621382029\n",
      "Iter 2310\n",
      "0.707651262664\n",
      "Iter 2320\n",
      "0.709755251387\n",
      "Iter 2330\n",
      "0.704560877421\n",
      "Iter 2340\n",
      "0.709153010139\n",
      "Iter 2350\n",
      "0.708625502797\n",
      "Iter 2360\n",
      "0.707038536846\n",
      "Iter 2370\n",
      "0.706566855827\n",
      "Iter 2380\n",
      "0.705694546471\n",
      "Iter 2390\n",
      "0.702162744892\n",
      "Iter 2400\n",
      "0.70689342287\n",
      "Iter 2410\n",
      "0.70373428814\n",
      "Iter 2420\n",
      "0.70908165336\n",
      "Iter 2430\n",
      "0.709031296602\n",
      "Iter 2440\n",
      "0.706593360333\n",
      "Iter 2450\n",
      "0.708994447458\n",
      "Iter 2460\n",
      "0.708451444162\n",
      "Iter 2470\n",
      "0.709469785321\n",
      "Iter 2480\n",
      "0.706853095768\n",
      "Iter 2490\n",
      "0.708827293147\n",
      "Iter 2500\n",
      "0.709695187386\n",
      "Iter 2510\n",
      "0.708675955922\n",
      "Iter 2520\n",
      "0.704434163713\n",
      "Iter 2530\n",
      "0.710603516632\n",
      "Iter 2540\n",
      "0.710778807754\n",
      "Iter 2550\n",
      "0.710259708702\n",
      "Iter 2560\n",
      "0.708882683511\n",
      "Iter 2570\n",
      "0.708491324303\n",
      "Iter 2580\n",
      "0.703135075868\n",
      "Iter 2590\n",
      "0.709163419759\n",
      "Iter 2600\n",
      "0.70619201047\n",
      "Iter 2610\n",
      "0.708503410162\n",
      "Iter 2620\n",
      "0.709522813309\n",
      "Iter 2630\n",
      "0.705668297444\n",
      "Iter 2640\n",
      "0.706495349468\n",
      "Iter 2650\n",
      "0.709548622921\n",
      "Iter 2660\n",
      "0.710388954249\n",
      "Iter 2670\n",
      "0.710279755196\n",
      "Iter 2680\n",
      "0.709002129544\n",
      "Iter 2690\n",
      "0.710485609031\n",
      "Iter 2700\n",
      "0.703983699929\n",
      "Iter 2710\n",
      "0.710357470204\n",
      "Iter 2720\n",
      "0.708963959067\n",
      "Iter 2730\n",
      "0.704981731667\n",
      "Iter 2740\n",
      "0.704407561961\n",
      "Iter 2750\n",
      "0.707674953985\n",
      "Iter 2760\n",
      "0.705497581886\n",
      "Iter 2770\n",
      "0.709765811805\n",
      "Iter 2780\n",
      "0.705630940419\n",
      "Iter 2790\n",
      "0.706977758078\n",
      "Iter 2800\n",
      "0.707374553534\n",
      "Iter 2810\n",
      "0.704252038514\n",
      "Iter 2820\n",
      "0.705726630837\n",
      "Iter 2830\n",
      "0.71018517014\n",
      "Iter 2840\n",
      "0.710128146626\n",
      "Iter 2850\n",
      "0.704383004759\n",
      "Iter 2860\n",
      "0.708646423433\n",
      "Iter 2870\n",
      "0.706632927986\n",
      "Iter 2880\n",
      "0.71148526143\n",
      "Iter 2890\n",
      "0.708212043631\n",
      "Iter 2900\n",
      "0.71123130797\n",
      "Iter 2910\n",
      "0.711045156896\n",
      "Iter 2920\n",
      "0.708863795592\n",
      "Iter 2930\n",
      "0.711468114789\n",
      "Iter 2940\n",
      "0.711718050522\n",
      "Iter 2950\n",
      "0.711822157092\n",
      "Iter 2960\n",
      "0.708751853778\n",
      "Iter 2970\n",
      "0.713200370302\n",
      "Iter 2980\n",
      "0.711326624225\n",
      "Iter 2990\n",
      "0.713752155942\n",
      "Iter 3000\n",
      "0.714306939064\n",
      "Iter 3010\n",
      "0.71264393822\n",
      "Iter 3020\n",
      "0.709396053758\n",
      "Iter 3030\n",
      "0.707922981313\n",
      "Iter 3040\n",
      "0.709033047924\n",
      "Iter 3050\n",
      "0.708480913672\n",
      "Iter 3060\n",
      "0.711307461381\n",
      "Iter 3070\n",
      "0.709354056999\n",
      "Iter 3080\n",
      "0.71227474296\n",
      "Iter 3090\n",
      "0.708737615537\n",
      "Iter 3100\n",
      "0.711198546949\n",
      "Iter 3110\n",
      "0.702374053514\n",
      "Iter 3120\n",
      "0.710007802757\n",
      "Iter 3130\n",
      "0.708341250606\n",
      "Iter 3140\n",
      "0.711772786348\n",
      "Iter 3150\n",
      "0.708197483703\n",
      "Iter 3160\n",
      "0.707927652849\n",
      "Iter 3170\n",
      "0.710575892434\n",
      "Iter 3180\n",
      "0.70678335757\n",
      "Iter 3190\n",
      "0.711846075548\n",
      "Iter 3200\n",
      "0.704704644315\n",
      "Iter 3210\n",
      "0.711980434397\n",
      "Iter 3220\n",
      "0.712412173735\n",
      "Iter 3230\n",
      "0.711961124862\n",
      "Iter 3240\n",
      "0.709600301614\n",
      "Iter 3250\n",
      "0.713765533233\n",
      "Iter 3260\n",
      "0.713123068137\n",
      "Iter 3270\n",
      "0.708996160842\n",
      "Iter 3280\n",
      "0.710590080572\n",
      "Iter 3290\n",
      "0.710675802291\n",
      "Iter 3300\n",
      "0.712675397799\n",
      "Iter 3310\n",
      "0.712266612692\n",
      "Iter 3320\n",
      "0.711198329633\n",
      "Iter 3330\n",
      "0.712217287757\n",
      "Iter 3340\n",
      "0.712554042849\n",
      "Iter 3350\n",
      "0.714103008038\n",
      "Iter 3360\n",
      "0.710414034662\n",
      "Iter 3370\n",
      "0.711345533863\n",
      "Iter 3380\n",
      "0.70866286488\n",
      "Iter 3390\n",
      "0.712131543088\n",
      "Iter 3400\n",
      "0.712745583905\n",
      "Iter 3410\n",
      "0.707603969093\n",
      "Iter 3420\n",
      "0.708581516519\n",
      "Iter 3430\n",
      "0.708210672248\n",
      "Iter 3440\n",
      "0.713443103078\n",
      "Iter 3450\n",
      "0.712877244079\n",
      "Iter 3460\n",
      "0.711478237937\n",
      "Iter 3470\n",
      "0.711942084303\n",
      "Iter 3480\n",
      "0.71398463511\n",
      "Iter 3490\n",
      "0.71561019118\n",
      "Iter 3500\n",
      "0.714596632252\n",
      "Iter 3510\n",
      "0.712079529011\n",
      "Iter 3520\n",
      "0.710368895096\n",
      "Iter 3530\n",
      "0.711634750853\n",
      "Iter 3540\n",
      "0.711853828428\n",
      "Iter 3550\n",
      "0.706363425292\n",
      "Iter 3560\n",
      "0.708943889622\n",
      "Iter 3570\n",
      "0.70230925881\n",
      "Iter 3580\n",
      "0.713073755167\n",
      "Iter 3590\n",
      "0.712076533905\n",
      "Iter 3600\n",
      "0.714513948577\n",
      "Iter 3610\n",
      "0.714709112058\n",
      "Iter 3620\n",
      "0.70990344269\n",
      "Iter 3630\n",
      "0.705088030479\n",
      "Iter 3640\n",
      "0.711266413249\n",
      "Iter 3650\n",
      "0.710078669589\n",
      "Iter 3660\n",
      "0.714859389746\n",
      "Iter 3670\n",
      "0.713914038456\n",
      "Iter 3680\n",
      "0.71326275238\n",
      "Iter 3690\n",
      "0.715972991518\n",
      "Iter 3700\n",
      "0.715249814605\n",
      "Iter 3710\n",
      "0.714249645507\n",
      "Iter 3720\n",
      "0.711684770431\n",
      "Iter 3730\n",
      "0.712812981633\n",
      "Iter 3740\n",
      "0.714077183254\n",
      "Iter 3750\n",
      "0.713569627804\n",
      "Iter 3760\n",
      "0.711566302041\n",
      "Iter 3770\n",
      "0.717246219474\n",
      "Iter 3780\n",
      "0.715410283224\n",
      "Iter 3790\n",
      "0.716549112296\n",
      "Iter 3800\n",
      "0.715076439688\n",
      "Iter 3810\n",
      "0.71376200747\n",
      "Iter 3820\n",
      "0.714092866534\n",
      "Iter 3830\n",
      "0.715329050436\n",
      "Iter 3840\n",
      "0.715095878904\n",
      "Iter 3850\n",
      "0.705495173068\n",
      "Iter 3860\n",
      "0.712710718332\n",
      "Iter 3870\n",
      "0.707220033442\n",
      "Iter 3880\n",
      "0.714484369946\n",
      "Iter 3890\n",
      "0.712496536101\n",
      "Iter 3900\n",
      "0.71356240693\n",
      "Iter 3910\n",
      "0.716212061249\n",
      "Iter 3920\n",
      "0.714560417847\n",
      "Iter 3930\n",
      "0.715867117687\n",
      "Iter 3940\n",
      "0.71460862712\n",
      "Iter 3950\n",
      "0.715686425459\n",
      "Iter 3960\n",
      "0.716550817054\n",
      "Iter 3970\n",
      "0.718953053433\n",
      "Iter 3980\n",
      "0.716745873222\n",
      "Iter 3990\n",
      "0.717947983887\n",
      "Iter 4000\n",
      "0.714786812215\n",
      "Iter 4010\n",
      "0.716268045014\n",
      "Iter 4020\n",
      "0.714587821432\n",
      "Iter 4030\n",
      "0.7111327066\n",
      "Iter 4040\n",
      "0.71808537587\n",
      "Iter 4050\n",
      "0.716503209817\n",
      "Iter 4060\n",
      "0.718074281097\n",
      "Iter 4070\n",
      "0.716433962336\n",
      "Iter 4080\n",
      "0.716815916455\n",
      "Iter 4090\n",
      "0.711517195616\n",
      "Iter 4100\n",
      "0.714711119433\n",
      "Iter 4110\n",
      "0.714029995583\n",
      "Iter 4120\n",
      "0.718229015629\n",
      "Iter 4130\n",
      "0.717804080082\n",
      "Iter 4140\n",
      "0.715947537001\n",
      "Iter 4150\n",
      "0.715633310986\n",
      "Iter 4160\n",
      "0.712237947641\n",
      "Iter 4170\n",
      "0.714818749687\n",
      "Iter 4180\n",
      "0.716667602849\n",
      "Iter 4190\n",
      "0.714801675391\n",
      "Iter 4200\n",
      "0.717375292127\n",
      "Iter 4210\n",
      "0.717177418069\n",
      "Iter 4220\n",
      "0.716171947414\n",
      "Iter 4230\n",
      "0.718694414301\n",
      "Iter 4240\n",
      "0.717895267853\n",
      "Iter 4250\n",
      "0.718002566637\n",
      "Iter 4260\n",
      "0.716171359872\n",
      "Iter 4270\n",
      "0.719940390812\n",
      "Iter 4280\n",
      "0.719319854564\n",
      "Iter 4290\n",
      "0.718621575519\n",
      "Iter 4300\n",
      "0.717556794223\n",
      "Iter 4310\n",
      "0.72031685755\n",
      "Iter 4320\n",
      "0.718830722094\n",
      "Iter 4330\n",
      "0.717302031821\n",
      "Iter 4340\n",
      "0.721552227417\n",
      "Iter 4350\n",
      "0.722473172299\n",
      "Iter 4360\n",
      "0.721177901899\n",
      "Iter 4370\n",
      "0.718059372444\n",
      "Iter 4380\n",
      "0.714961294399\n",
      "Iter 4390\n",
      "0.711330084983\n",
      "Iter 4400\n",
      "0.714147780662\n",
      "Iter 4410\n",
      "0.715538465275\n",
      "Iter 4420\n",
      "0.712519022321\n",
      "Iter 4430\n",
      "0.719354320165\n",
      "Iter 4440\n",
      "0.716858205683\n",
      "Iter 4450\n",
      "0.718869198768\n",
      "Iter 4460\n",
      "0.717413211426\n",
      "Iter 4470\n",
      "0.713859775813\n",
      "Iter 4480\n",
      "0.714860938584\n",
      "Iter 4490\n",
      "0.715407635776\n",
      "Iter 4500\n",
      "0.719697817739\n",
      "Iter 4510\n",
      "0.720221355574\n",
      "Iter 4520\n",
      "0.719248646694\n",
      "Iter 4530\n",
      "0.715930758172\n",
      "Iter 4540\n",
      "0.717487199178\n",
      "Iter 4550\n",
      "0.711397302986\n",
      "Iter 4560\n",
      "0.715566666928\n",
      "Iter 4570\n",
      "0.716428169856\n",
      "Iter 4580\n",
      "0.718668113296\n",
      "Iter 4590\n",
      "0.720095312076\n",
      "Iter 4600\n",
      "0.723210825423\n",
      "Iter 4610\n",
      "0.72121649498\n",
      "Iter 4620\n",
      "0.720469226291\n",
      "Iter 4630\n",
      "0.721950989817\n",
      "Iter 4640\n",
      "0.721232724607\n",
      "Iter 4650\n",
      "0.721851635889\n",
      "Iter 4660\n",
      "0.719394982015\n",
      "Iter 4670\n",
      "0.711063958377\n",
      "Iter 4680\n",
      "0.715234157081\n",
      "Iter 4690\n",
      "0.719768087697\n",
      "Iter 4700\n",
      "0.713927812922\n",
      "Iter 4710\n",
      "0.718410152876\n",
      "Iter 4720\n",
      "0.71787482918\n",
      "Iter 4730\n",
      "0.70743224128\n",
      "Iter 4740\n",
      "0.714968254235\n",
      "Iter 4750\n",
      "0.717417666511\n",
      "Iter 4760\n",
      "0.719321997894\n",
      "Iter 4770\n",
      "0.71627420412\n",
      "Iter 4780\n",
      "0.717071031514\n",
      "Iter 4790\n",
      "0.709991820794\n",
      "Iter 4800\n",
      "0.720963161155\n",
      "Iter 4810\n",
      "0.717098242716\n",
      "Iter 4820\n",
      "0.714804373801\n",
      "Iter 4830\n",
      "0.703953902636\n",
      "Iter 4840\n",
      "0.712978068232\n",
      "Iter 4850\n",
      "0.717772127504\n",
      "Iter 4860\n",
      "0.719001516205\n",
      "Iter 4870\n",
      "0.718253049605\n",
      "Iter 4880\n",
      "0.718110447598\n",
      "Iter 4890\n",
      "0.717767679569\n",
      "Iter 4900\n",
      "0.7162245653\n",
      "Iter 4910\n",
      "0.712109947362\n",
      "Iter 4920\n",
      "0.715840897234\n",
      "Iter 4930\n",
      "0.719111944981\n",
      "Iter 4940\n",
      "0.722633025731\n",
      "Iter 4950\n",
      "0.714729935091\n",
      "Iter 4960\n",
      "0.723206962077\n",
      "Iter 4970\n",
      "0.719143163364\n",
      "Iter 4980\n",
      "0.72369231537\n",
      "Iter 4990\n",
      "0.721973308337\n",
      "Iter 5000\n",
      "0.720376560656\n",
      "Iter 5010\n",
      "0.712399053525\n",
      "Iter 5020\n",
      "0.718655499456\n",
      "Iter 5030\n",
      "0.715048449399\n",
      "Iter 5040\n",
      "0.710727771384\n",
      "Iter 5050\n",
      "0.720622424467\n",
      "Iter 5060\n",
      "0.720392385543\n",
      "Iter 5070\n",
      "0.718136198477\n",
      "Iter 5080\n",
      "0.71708933485\n",
      "Iter 5090\n",
      "0.718442706232\n",
      "Iter 5100\n",
      "0.714985087644\n",
      "Iter 5110\n",
      "0.716997337294\n",
      "Iter 5120\n",
      "0.718415694907\n",
      "Iter 5130\n",
      "0.716321525429\n",
      "Iter 5140\n",
      "0.715842488864\n",
      "Iter 5150\n",
      "0.716596401433\n",
      "Iter 5160\n",
      "0.715911189711\n",
      "Iter 5170\n",
      "0.717582154392\n",
      "Iter 5180\n",
      "0.717081087798\n",
      "Iter 5190\n",
      "0.713734678097\n",
      "Iter 5200\n",
      "0.715399317879\n",
      "Iter 5210\n",
      "0.713831946426\n",
      "Iter 5220\n",
      "0.715270691524\n",
      "Iter 5230\n",
      "0.715138934522\n",
      "Iter 5240\n",
      "0.704742646633\n",
      "Iter 5250\n",
      "0.712892655406\n",
      "Iter 5260\n",
      "0.716954932995\n",
      "Iter 5270\n",
      "0.717096116595\n",
      "Iter 5280\n",
      "0.705960519276\n",
      "Iter 5290\n",
      "0.712202608945\n",
      "Iter 5300\n",
      "0.716081835629\n",
      "Iter 5310\n",
      "0.705146192659\n",
      "Iter 5320\n",
      "0.715392751859\n",
      "Iter 5330\n",
      "0.714384906709\n",
      "Iter 5340\n",
      "0.717015426125\n",
      "Iter 5350\n",
      "0.7118403274\n",
      "Iter 5360\n",
      "0.718871882123\n",
      "Iter 5370\n",
      "0.718067906381\n",
      "Iter 5380\n",
      "0.718743100769\n",
      "Iter 5390\n",
      "0.722223778851\n",
      "Iter 5400\n",
      "0.710528257294\n",
      "Iter 5410\n",
      "0.706526032916\n",
      "Iter 5420\n",
      "0.712371058474\n",
      "Iter 5430\n",
      "0.712161180316\n",
      "Iter 5440\n",
      "0.713243396295\n",
      "Iter 5450\n",
      "0.717333753308\n",
      "Iter 5460\n",
      "0.722459096401\n",
      "Iter 5470\n",
      "0.716658012594\n",
      "Iter 5480\n",
      "0.715733486597\n",
      "Iter 5490\n",
      "0.719680934114\n",
      "Iter 5500\n",
      "0.724284416913\n",
      "Iter 5510\n",
      "0.716142805072\n",
      "Iter 5520\n",
      "0.72057989036\n",
      "Iter 5530\n",
      "0.718726618664\n",
      "Iter 5540\n",
      "0.71651891284\n",
      "Iter 5550\n",
      "0.717292160007\n",
      "Iter 5560\n",
      "0.719522587181\n",
      "Iter 5570\n",
      "0.718996658226\n",
      "Iter 5580\n",
      "0.709525851636\n",
      "Iter 5590\n",
      "0.714284182435\n",
      "Iter 5600\n",
      "0.716837347987\n",
      "Iter 5610\n",
      "0.709799500276\n",
      "Iter 5620\n",
      "0.712553118555\n",
      "Iter 5630\n",
      "0.718997436065\n",
      "Iter 5640\n",
      "0.721337475623\n",
      "Iter 5650\n",
      "0.716455270514\n",
      "Iter 5660\n",
      "0.70991372325\n",
      "Iter 5670\n",
      "0.718146971314\n",
      "Iter 5680\n",
      "0.722662403767\n",
      "Iter 5690\n",
      "0.719422841739\n",
      "Iter 5700\n",
      "0.71737069843\n",
      "Iter 5710\n",
      "0.717657843786\n",
      "Iter 5720\n",
      "0.71742195423\n",
      "Iter 5730\n",
      "0.716665874129\n",
      "Iter 5740\n",
      "0.715989648068\n",
      "Iter 5750\n",
      "0.715067063228\n",
      "Iter 5760\n",
      "0.717651083531\n",
      "Iter 5770\n",
      "0.708958235848\n",
      "Iter 5780\n",
      "0.709453175985\n",
      "Iter 5790\n",
      "0.705732998283\n",
      "Iter 5800\n",
      "0.718253543328\n",
      "Iter 5810\n",
      "0.712020047137\n",
      "Iter 5820\n",
      "0.720232360063\n",
      "Iter 5830\n",
      "0.718730589236\n",
      "Iter 5840\n",
      "0.706027866694\n",
      "Iter 5850\n",
      "0.706356245419\n",
      "Iter 5860\n",
      "0.708177740645\n",
      "Iter 5870\n",
      "0.719597629996\n",
      "Iter 5880\n",
      "0.716469148731\n",
      "Iter 5890\n",
      "0.711268816425\n",
      "Iter 5900\n",
      "0.713896493722\n",
      "Iter 5910\n",
      "0.718383753578\n",
      "Iter 5920\n",
      "0.718448592606\n",
      "Iter 5930\n",
      "0.716026944831\n",
      "Iter 5940\n",
      "0.719895778806\n",
      "Iter 5950\n",
      "0.710721901248\n",
      "Iter 5960\n",
      "0.709495840458\n",
      "Iter 5970\n",
      "0.69093842915\n",
      "Iter 5980\n",
      "0.716262873489\n",
      "Iter 5990\n",
      "0.71765690158\n",
      "[0.71072190124802315, 0.71176649387016733, 0.71302196437468879, 0.71494549774974425, 0.7159611306637601, 0.71569772505453222, 0.71519582745383281, 0.71377235550155071, 0.71204617995454977, 0.71003149561881074, 0.70949584045763436, 0.70894792221848779, 0.70781781088364537, 0.70672819112986196, 0.7052568432390357, 0.7028559765017256, 0.70068872895901113, 0.69741675045542528, 0.69396954405208067, 0.69215487914968199, 0.69093842914961234, 0.69227591722657711, 0.69456503389092872, 0.69861208342687275, 0.70341732272433399, 0.70942258607613384, 0.71381229960973236, 0.71616289841557146, 0.7169546807791094, 0.71668878033900985, 0.71626287348877538, 0.71543002186622662, 0.71457371898334865, 0.7143710648588546, 0.71483464612326086, 0.71526126603142071, 0.71576501428034334, 0.71651800236097829, 0.71709921805220267, 0.71715534101489309, 0.71765690158022033, 0.71777035570882797, 0.71764919230916691, 0.7179544808080327, 0.71741034014006355, 0.71565626654679604, 0.71330579108701408, 0.7104834952624407, 0.70754701482959403, 0.70799170246241294]\n",
      "total running time cost:3906.60241199s\n"
     ]
    }
   ],
   "source": [
    "## iterating among all customers to find current training customer\n",
    "accuracy = []\n",
    "result_final = []\n",
    "for i in range(1):\n",
    "    test_x_name = data_dir + 'test_x_' + str(i) + '.csv'\n",
    "    test_y_name = data_dir + 'test_y_' + str(i) + '.csv'\n",
    "    train_x_name = data_dir + 'train_x_' + str(i) + '.csv'\n",
    "    train_y_name = data_dir + 'train_y_' + str(i) + '.csv'\n",
    "    tmp_data = np.array(pd.read_csv(test_x_name,header = None))\n",
    "    test_x_data = tmp_data[:,1:]\n",
    "    # print test_x_data.dtype  data are stored as float64 double precision format\n",
    "    tmp_data = np.array(pd.read_csv(test_y_name,header = None))\n",
    "    test_y_data = tmp_data[:,1:]\n",
    "    tmp_data = np.array(pd.read_csv(train_x_name,header = None))\n",
    "    train_x_data = tmp_data[:,1:]\n",
    "    tmp_data = np.array(pd.read_csv(train_y_name,header = None))\n",
    "    train_y_data = tmp_data[:,1:]\n",
    "    #log them\n",
    "    #test_x_data = np.log(test_x_data)\n",
    "    #test_y_data = np.log(test_y_data)\n",
    "    #train_x_data = np.log(train_x_data)\n",
    "    #train_y_data = np.log(train_y_data)\n",
    "    \n",
    "    traindays = train_y_data.shape[0]\n",
    "    # generate test data\n",
    "    test_x,test_y = test_data_gen(test_x_data,test_y_data,n_steps)\n",
    "    test_x = test_x.reshape(test_batch_size,n_steps,feature_size)\n",
    "    ### Execute\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    outp = []\n",
    "    outlist = np.zeros([Rs,test_batch_size])\n",
    "    with tf.Session() as sess:\n",
    "        # Create a summary to monitor cost function\n",
    "        tf.scalar_summary(\"loss\", cost)\n",
    "        # Merge all summaries to a single operator\n",
    "        merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "        # tensorboard info.# Set logs writer into folder /tmp/tensorflow_logs\n",
    "        summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def)\n",
    "\n",
    "        #initialize all variables in the model\n",
    "        sess.run(init)\n",
    "        for k in range(num_epoches):\n",
    "            #Generate Data for each epoch\n",
    "            #What this does is it creates a list of of elements of length seq_len, each of size [batch_size,input_size]\n",
    "            #this is required to feed data into rnn.rnn\n",
    "            #print traindays\n",
    "            X,Y = train_data_gen(traindays,train_x_data,train_y_data,n_steps)\n",
    "            X = X.reshape(train_batch_size,n_steps,feature_size)\n",
    "\n",
    "\n",
    "            #Create the dictionary of inputs to feed into sess.run\n",
    "            #if k < 0:\n",
    "            #    sess.run(optimizer2,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})\n",
    "            #else:\n",
    "            sess.run(optimizer,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})   \n",
    "            #perform an update on the parameters\n",
    "            \n",
    "            if k >= num_epoches-Rs:\n",
    "                output_tmp = sess.run(pred,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "                outp_test = output_tmp\n",
    "                outlist[k-num_epoches+Rs,:] = outp_test.copy().T\n",
    "\n",
    "            # Write logs at every iteration\n",
    "            if k>50 & k%10 == 0:\n",
    "                summary_str = sess.run(merged_summary_op, feed_dict={x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "                summary_writer.add_summary(summary_str, k)\n",
    "            \n",
    "            if k % 10 == 0:\n",
    "                output_tmp_ex = sess.run(pred,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "                print \"Iter \" + str(k)# + \", Minibatch Loss ---- Train = \" + \"{:.6f}\".format(loss1) + \"; Test = \" + \"{:.6f}\".format(loss2)\n",
    "        #print \"haha{}\".format(outp)\n",
    "                ktmp = np.corrcoef(output_tmp_ex.T,test_y.T)[0,1]\n",
    "                accuracy.append(ktmp)\n",
    "                print ktmp\n",
    "    R = []\n",
    "    RR  = []\n",
    "    for i in range(Rs):\n",
    "        out = np.array(outlist[i])\n",
    "        #R.append(np.corrcoef(np.exp(out.T),np.exp(test_y.T))[0,1])\n",
    "        R.append(np.corrcoef(out.T,test_y.T)[0,1])\n",
    "        RR.append(np.corrcoef(out.T,test_y.T)[0,1]**2)\n",
    "    print R\n",
    "    RRR = np.mean(R)# average Rs R in this time of train\n",
    "    \n",
    "    \n",
    "    # run time\n",
    "    time2 = time.time()\n",
    "    print 'total running time cost:{}s'.format(time2-time1)\n",
    "    \n",
    "    # append R\n",
    "    result_final.append(RRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.71024079595938017]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(accuracy).to_csv('R-house0-size-3-20-adam.csv')\n",
    "result_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw\n",
    "#xxx = np.arange(0,test_batch_size)\n",
    "#pl.plot(xxx,out,color = \"red\")\n",
    "#pl.plot(xxx,test_y)\n",
    "#pl.grid()\n",
    "##pl.legend()\n",
    "#pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#out = np.array(outlist[0])\n",
    "#np.corrcoef(out.T,test_y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DataFrame(result_final).to_csv('result2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a = np.array(a).reshape((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#b = np.log(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#c = np.exp(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdamOptimizer in module tensorflow.python.training.adam:\n",
      "\n",
      "class AdamOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      " |  Optimizer that implements the Adam algorithm.\n",
      " |  \n",
      " |  See [Kingma et. al., 2014](http://arxiv.org/abs/1412.6980)\n",
      " |  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n",
      " |  \n",
      " |  @@__init__\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdamOptimizer\n",
      " |      tensorflow.python.training.optimizer.Optimizer\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
      " |      Construct a new Adam optimizer.\n",
      " |      \n",
      " |      Initialization:\n",
      " |      \n",
      " |      ```\n",
      " |      m_0 <- 0 (Initialize initial 1st moment vector)\n",
      " |      v_0 <- 0 (Initialize initial 2nd moment vector)\n",
      " |      t <- 0 (Initialize timestep)\n",
      " |      ```\n",
      " |      \n",
      " |      The update rule for `variable` with gradient `g` uses an optimization\n",
      " |      described at the end of section2 of the paper:\n",
      " |      \n",
      " |      ```\n",
      " |      t <- t + 1\n",
      " |      lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
      " |      \n",
      " |      m_t <- beta1 * m_{t-1} + (1 - beta1) * g\n",
      " |      v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\n",
      " |      variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n",
      " |      ```\n",
      " |      \n",
      " |      The default value of 1e-8 for epsilon might not be a good default in\n",
      " |      general. For example, when training an Inception network on ImageNet a\n",
      " |      current good choice is 1.0 or 0.1.\n",
      " |      \n",
      " |      Args:\n",
      " |        learning_rate: A Tensor or a floating point value.  The learning rate.\n",
      " |        beta1: A float value or a constant float tensor.\n",
      " |          The exponential decay rate for the 1st moment estimates.\n",
      " |        beta2: A float value or a constant float tensor.\n",
      " |          The exponential decay rate for the 2nd moment estimates.\n",
      " |        epsilon: A small constant for numerical stability.\n",
      " |        use_locking: If True use locks for update operations.\n",
      " |        name: Optional name for the operations created when applying gradients.\n",
      " |          Defaults to \"Adam\".\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      " |      applies gradients.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      " |          `compute_gradients()`.\n",
      " |        global_step: Optional `Variable` to increment by one after the\n",
      " |          variables have been updated.\n",
      " |        name: Optional name for the returned operation.  Default to the\n",
      " |          name passed to the `Optimizer` constructor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that applies the specified gradients. If `global_step`\n",
      " |        was not None, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        ValueError: If none of the variables have gradients.\n",
      " |  \n",
      " |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False)\n",
      " |      Compute gradients of `loss` for the variables in `var_list`.\n",
      " |      \n",
      " |      This is the first part of `minimize()`.  It returns a list\n",
      " |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      " |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      " |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      " |      given variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A Tensor containing the value to minimize.\n",
      " |        var_list: Optional list of tf.Variable to update to minimize\n",
      " |          `loss`.  Defaults to the list of variables collected in the graph\n",
      " |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      " |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      " |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      " |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      " |          Valid values are defined in the class `AggregationMethod`.\n",
      " |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      " |          the corresponding op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of (gradient, variable) pairs.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      " |        ValueError: If some arguments are invalid.\n",
      " |  \n",
      " |  get_slot(self, var, name)\n",
      " |      Return a slot named `name` created for `var` by the Optimizer.\n",
      " |      \n",
      " |      Some `Optimizer` subclasses use additional variables.  For example\n",
      " |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      " |      gives access to these `Variable` objects if for some reason you need them.\n",
      " |      \n",
      " |      Use `get_slot_names()` to get the list of slot names created by the\n",
      " |      `Optimizer`.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      " |        name: A string.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      " |  \n",
      " |  get_slot_names(self)\n",
      " |      Return a list of the names of slots created by the `Optimizer`.\n",
      " |      \n",
      " |      See `get_slot()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of strings.\n",
      " |  \n",
      " |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None)\n",
      " |      Add operations to minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply combines calls `compute_gradients()` and\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A `Tensor` containing the value to minimize.\n",
      " |        global_step: Optional `Variable` to increment by one after the\n",
      " |          variables have been updated.\n",
      " |        var_list: Optional list of `Variable` objects to update to minimize\n",
      " |          `loss`.  Defaults to the list of variables collected in the graph\n",
      " |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      " |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      " |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      " |          Valid values are defined in the class `AggregationMethod`.\n",
      " |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      " |          the corresponding op.\n",
      " |        name: Optional name for the returned operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      " |        was not `None`, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If some of the variables are not `Variable` objects.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  GATE_GRAPH = 2\n",
      " |  \n",
      " |  GATE_NONE = 0\n",
      " |  \n",
      " |  GATE_OP = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
