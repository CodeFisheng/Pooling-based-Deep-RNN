{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning (RNN) Demo for Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: setting all global parameters -- sec 2 network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time1 = time.time() # set up counter to record run time\n",
    "data_dir = './data/' # directory contains input data\n",
    "num_epoches = 5000 # training epoches for each customer samples\n",
    "n_steps = 48 # input size\n",
    "test_batch_size = 70*48 # days of a batch\n",
    "train_batch_size = 2*48\n",
    "feature_size = 1 # same time of a week\n",
    "n_hidden = 30 # input size\n",
    "num_layers = 5\n",
    "n_output = 1\n",
    "Rs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 4: define data generating function code. \n",
    "which generate a batch of batch-size large sequence data. the data is feature_size dims width and is a time series of float32 of steps steps. inputs and outputs are:\n",
    "\n",
    "inputs:\n",
    "----n_batch: number of samples in a batch\n",
    "----steps: the sequence length of a sample data\n",
    "----feature_size: dimensions of a single time step data frame\n",
    "\n",
    "outputs:\n",
    "----X inputs, shape(n_batch,steps,feature_size)\n",
    "----Y outputs should be, shape(n_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_gen(totaltraindays,x_data,y_data,steps = 48, n_batch = train_batch_size):\n",
    "    X = np.zeros((n_batch,steps,feature_size))\n",
    "    Y = np.zeros((n_batch,feature_size))\n",
    "    rang = range(totaltraindays) # test day sample range\n",
    "    train_days_list = rd.sample(rang,n_batch) # pick unduplicated n indexes as examples\n",
    "    #print totaltraindays\n",
    "    tmpX = [x_data[i,0-steps:] for i in train_days_list]\n",
    "    tmpY = [y_data[i,:] for i in train_days_list]\n",
    "    X = np.array(tmpX).reshape(n_batch,steps,feature_size)\n",
    "    Y = np.array(tmpY).reshape(n_batch,feature_size)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data_gen(x_data,y_data,steps = 48, n_batch = test_batch_size):\n",
    "    X = np.zeros((n_batch,steps,feature_size))\n",
    "    Y = np.zeros((n_batch,feature_size))\n",
    "    #print x_data[:,0-steps:].shape,y_data.shape\n",
    "    #print n_batch, steps\n",
    "    X = x_data[:,0-steps:].reshape(n_batch,steps,feature_size)\n",
    "    Y = y_data.reshape(n_batch,feature_size)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: construct RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create placeholder for x and y\n",
    "x = tf.placeholder(\"float\",[None,n_steps,feature_size])\n",
    "istate = tf.placeholder(\"float\",[None,num_layers*2*n_hidden])\n",
    "y = tf.placeholder(\"float\",[None,n_output])\n",
    "\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([feature_size, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_output]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(_X, _istate, _weights, _biases):\n",
    "\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, feature_size]) # (n_steps*batch_size, n_input)\n",
    "    # Linear activation\n",
    "    _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    stacked_lstm_cell = rnn_cell.MultiRNNCell([lstm_cell]*num_layers)\n",
    "    \n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(stacked_lstm_cell, _X, initial_state=_istate)\n",
    "\n",
    "    # Linear activation\n",
    "    # Get inner loop last output\n",
    "    return tf.matmul(outputs[-1], _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = RNN(x, istate, weights, biases)\n",
    "\n",
    "#cost function \n",
    "cost = tf.reduce_mean(tf.pow(pred-y,2)) # cost function of this batch of data\n",
    "#compute parameter updates\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer2 = tf.train.RMSPropOptimizer(0.005, 0.3).minimize(cost2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxe(predictions, targets):\n",
    "    return max(abs(predictions-targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 ---- Process: 0.00%\n",
      "Iter 10 ---- Process: 2.00%\n",
      "Iter 20 ---- Process: 4.00%\n",
      "Iter 30 ---- Process: 6.00%\n",
      "Iter 40 ---- Process: 8.00%\n",
      "Iter 50 ---- Process: 10.00%\n",
      "Iter 60 ---- Process: 12.00%\n",
      "Iter 70 ---- Process: 14.00%\n",
      "Iter 80 ---- Process: 16.00%\n",
      "Iter 90 ---- Process: 18.00%\n",
      "Iter 100 ---- Process: 20.00%\n",
      "Iter 110 ---- Process: 22.00%\n",
      "Iter 120 ---- Process: 24.00%\n",
      "Iter 130 ---- Process: 26.00%\n",
      "Iter 140 ---- Process: 28.00%\n",
      "Iter 150 ---- Process: 30.00%\n",
      "Iter 160 ---- Process: 32.00%\n",
      "Iter 170 ---- Process: 34.00%\n",
      "Iter 180 ---- Process: 36.00%\n",
      "Iter 190 ---- Process: 38.00%\n",
      "Iter 200 ---- Process: 40.00%\n",
      "Iter 210 ---- Process: 42.00%\n",
      "Iter 220 ---- Process: 44.00%\n",
      "Iter 230 ---- Process: 46.00%\n",
      "Iter 240 ---- Process: 48.00%\n",
      "Iter 250 ---- Process: 50.00%\n",
      "Iter 260 ---- Process: 52.00%\n",
      "Iter 270 ---- Process: 54.00%\n",
      "Iter 280 ---- Process: 56.00%\n",
      "Iter 290 ---- Process: 58.00%\n",
      "Iter 300 ---- Process: 60.00%\n",
      "Iter 310 ---- Process: 62.00%\n",
      "Iter 320 ---- Process: 64.00%\n",
      "Iter 330 ---- Process: 66.00%\n",
      "Iter 340 ---- Process: 68.00%\n",
      "Iter 350 ---- Process: 70.00%\n",
      "Iter 360 ---- Process: 72.00%\n",
      "Iter 370 ---- Process: 74.00%\n",
      "Iter 380 ---- Process: 76.00%\n",
      "Iter 390 ---- Process: 78.00%\n",
      "Iter 400 ---- Process: 80.00%\n",
      "Iter 410 ---- Process: 82.00%\n",
      "Iter 420 ---- Process: 84.00%\n",
      "Iter 430 ---- Process: 86.00%\n",
      "Iter 440 ---- Process: 88.00%\n",
      "Iter 450 ---- Process: 90.00%\n",
      "Iter 460 ---- Process: 92.00%\n",
      "Iter 470 ---- Process: 94.00%\n",
      "Iter 480 ---- Process: 96.00%\n",
      "Iter 490 ---- Process: 98.00%\n"
     ]
    }
   ],
   "source": [
    "## iterating among all customers to find current training customer\n",
    "#cus_list = [4,5,8,9]\n",
    "outlist = np.zeros([(num_epoches/10),test_batch_size])\n",
    "kind = 0\n",
    "cus_list = [8,9,11,18,29,45,48,49,58,60,64,65,66,68]\n",
    "for i in range(0,14):\n",
    "    accuracy = []\n",
    "    accuracy1 = []\n",
    "    ii = cus_list[i]\n",
    "    test_x_name = data_dir + 'test_x_' + str(ii) + '.csv'\n",
    "    test_y_name = data_dir + 'test_y_' + str(ii) + '.csv'\n",
    "    train_x_name = data_dir + 'train_x_' + str(ii) + '.csv'\n",
    "    train_y_name = data_dir + 'train_y_' + str(ii) + '.csv'\n",
    "    tmp_data = np.array(pd.read_csv(test_x_name,header = None))\n",
    "    test_x_data = tmp_data[:,1:]\n",
    "    # print test_x_data.dtype  data are stored as float64 double precision format\n",
    "    tmp_data = np.array(pd.read_csv(test_y_name,header = None))\n",
    "    test_y_data = tmp_data[:,1:]\n",
    "    tmp_data = np.array(pd.read_csv(train_x_name,header = None))\n",
    "    train_x_data = tmp_data[:,1:]\n",
    "    tmp_data = np.array(pd.read_csv(train_y_name,header = None))\n",
    "    train_y_data = tmp_data[:,1:]\n",
    "    #log them\n",
    "    #test_x_data = np.log(test_x_data)\n",
    "    #test_y_data = np.log(test_y_data)\n",
    "    #train_x_data = np.log(train_x_data)\n",
    "    #train_y_data = np.log(train_y_data)\n",
    "    \n",
    "    traindays = train_y_data.shape[0]\n",
    "    # generate test data\n",
    "    test_x,test_y = test_data_gen(test_x_data,test_y_data,n_steps)\n",
    "    test_x = test_x.reshape(test_batch_size,n_steps,feature_size)\n",
    "    ### Execute\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:\n",
    "        # Create a summary to monitor cost function\n",
    "        #tf.scalar_summary(\"loss\", cost)\n",
    "        # Merge all summaries to a single operator\n",
    "        #merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "        # tensorboard info.# Set logs writer into folder /tmp/tensorflow_logs\n",
    "        #summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def)\n",
    "\n",
    "        #initialize all variables in the model\n",
    "        sess.run(init)\n",
    "        for k in range(num_epoches):\n",
    "            #Generate Data for each epoch\n",
    "            #What this does is it creates a list of of elements of length seq_len, each of size [batch_size,input_size]\n",
    "            #this is required to feed data into rnn.rnn\n",
    "            #print traindays\n",
    "            X,Y = train_data_gen(traindays,train_x_data,train_y_data,n_steps)\n",
    "            X = X.reshape(train_batch_size,n_steps,feature_size)\n",
    "\n",
    "\n",
    "            #Create the dictionary of inputs to feed into sess.run\n",
    "            #if k < 0:\n",
    "            #    sess.run(optimizer2,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})\n",
    "            #else:\n",
    "            sess.run(optimizer,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})   \n",
    "            #perform an update on the parameters\n",
    "\n",
    "            # Write logs at every iteration\n",
    "            #if k>50 & k%10 == 0:\n",
    "            #    summary_str = sess.run(merged_summary_op, feed_dict={x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "            #    summary_writer.add_summary(summary_str, k)\n",
    "            \n",
    "            #if k % 10 == 0:\n",
    "            if k % 10 == 0:\n",
    "                output_tmp_ex = sess.run(pred,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )  \n",
    "                print \"Iter \" + str(k) + \" ---- Process: \" + \"{:.2f}\".format(100*float(k)/float(num_epoches)) + \"%\"\n",
    "                outp_test = output_tmp_ex\n",
    "                outlist[kind,:] = outp_test.copy().T\n",
    "                kind = kind + 1\n",
    "            #    print ktmp\n",
    "            #if k % 10 == 0:\n",
    "            #    output_tmp_ex = sess.run(pred,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "            #    print \"Iter \" + str(k)# + \", Minibatch Loss ---- Train = \" + \"{:.6f}\".format(loss1) + \"; Test = \" + \"{:.6f}\".format(loss2)\n",
    "        #print \"haha{}\".format(outp)\n",
    "            #    ktmp = np.corrcoef(output_tmp_ex.T,test_y.T)[0,1]\n",
    "            #    accuracy1.append(ktmp)\n",
    "            #    print ktmp\n",
    "\n",
    "    ## evaluation\n",
    "    RList = np.zeros([(num_epoches/10)])\n",
    "    rmseList = np.zeros([(num_epoches/10)])\n",
    "    maxeList = np.zeros([(num_epoches/10)])\n",
    "    for i in range(kind):\n",
    "        out = np.array(outlist[i])\n",
    "        tmp = out.T.reshape((1,test_batch_size))\n",
    "        RList[i] = np.corrcoef(tmp[0,:],test_y.T[0,:])[0,1]\n",
    "        rmseList[i] = rmse(tmp[0,:],test_y.T[0,:])\n",
    "        maxeList[i] = maxe(tmp[0,:],test_y.T[0,:])\n",
    "    \n",
    "    ## serialize\n",
    "    prefix = './final_result/single/'\n",
    "    postfix = '-house-' + str(ii) + '-5-30-final.csv'\n",
    "    DataFrame(RList).to_csv(prefix+'RList'+postfix)\n",
    "    DataFrame(rmseList).to_csv(prefix+'rmseList'+postfix)\n",
    "    DataFrame(maxeList).to_csv(prefix+'maxeList'+postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.0367729663849"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time2 = time.time()\n",
    "time = time2-time1\n",
    "time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
