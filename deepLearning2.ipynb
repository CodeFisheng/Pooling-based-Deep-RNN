{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning (RNN) Demo for Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: setting all global parameters -- sec 1 data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "data_path = './input.csv'\n",
    "total_days = 350\n",
    "train_days = 280\n",
    "test_days = 70\n",
    "data_length = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: setting all global parameters -- sec 2 network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epoches = 100\n",
    "n_steps = 48 # input size\n",
    "batch_size = 70*48 # days of a batch\n",
    "feature_size = 1 # same time of a week\n",
    "n_hidden = 5 # input size\n",
    "num_layers = 2\n",
    "n_output = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (1, 13440), test data shape: (1, 3408)\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(data_path)\n",
    "dat = np.array(dataframe)\n",
    "date_list = dat[:,1]\n",
    "dat = dat[:,2:]# drop the first two cols --- index and date\n",
    "nrows,ncols = dat.shape\n",
    "#print nrows,ncols\n",
    "data = dat.reshape((1,nrows*ncols))\n",
    "data_length = data.shape[1]\n",
    "\n",
    "# construct training data\n",
    "train_len = train_days*48\n",
    "train_data = data[0,0:train_len]\n",
    "train_data = train_data.reshape([1,train_len])\n",
    "\n",
    "# construct testing data\n",
    "## test size = input_size + test days size. since, the output should be \n",
    "## from first test sample to last. prefix is input-size data\n",
    "test_len = test_days*48\n",
    "test_data = np.zeros(test_len+n_steps)\n",
    "test_data[n_steps:] = data[0,train_len:train_len+test_len]\n",
    "test_data[0:n_steps] = data[0,train_len-n_steps:train_len]\n",
    "test_data = test_data.reshape([1,test_len+n_steps])\n",
    "print \"train data shape: {}, test data shape: {}\".format(train_data.shape,test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 4: define data generating function code. \n",
    "which generate a batch of batch-size large sequence data. the data is feature_size dims width and is a time series of float32 of steps steps. inputs and outputs are:\n",
    "\n",
    "inputs:\n",
    "----n_batch: number of samples in a batch\n",
    "----steps: the sequence length of a sample data\n",
    "----feature_size: dimensions of a single time step data frame\n",
    "\n",
    "outputs:\n",
    "----X inputs, shape(n_batch,steps,feature_size)\n",
    "----Y outputs should be, shape(n_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_gen(steps = 48, n_batch = 48):\n",
    "    X = np.zeros((n_batch,steps,feature_size))\n",
    "    Y = np.zeros((n_batch,feature_size))\n",
    "    #for each n, compute X and correct y values\n",
    "    for n in range(n_batch):\n",
    "        # randomly pick a sample's y, between acceptable range\n",
    "        index = np.random.randint(steps,train_len)\n",
    "        # update y\n",
    "        Y[n] = train_data[0,index]\n",
    "        # update X from index-steps to index-1\n",
    "        X[n,:,:] = train_data[0,index-steps:index].T.reshape(steps,1)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data_gen(steps = 7*48, n_batch = 70*48):\n",
    "    X = np.zeros((n_batch,steps,feature_size))\n",
    "    Y = np.zeros((n_batch,feature_size))\n",
    "    #for each n, compute X and correct y values\n",
    "    for n in range(n_batch):\n",
    "        # randomly pick a sample's y, between acceptable range\n",
    "        index = steps\n",
    "        # update y\n",
    "        Y[n] = test_data[0,steps+n]\n",
    "        # update X from index-steps to index-1\n",
    "        X[n,:,:] = test_data[0,n:n+steps].T.reshape(steps,1)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: construct RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create placeholder for x and y\n",
    "x = tf.placeholder(\"float\",[None,n_steps,feature_size])\n",
    "istate = tf.placeholder(\"float\",[None,num_layers*2*n_hidden])\n",
    "y = tf.placeholder(\"float\",[None,n_output])\n",
    "\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([feature_size, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_output]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(_X, _istate, _weights, _biases):\n",
    "\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, feature_size]) # (n_steps*batch_size, n_input)\n",
    "    # Linear activation\n",
    "    _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    stacked_lstm_cell = rnn_cell.MultiRNNCell([lstm_cell]*num_layers)\n",
    "    \n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(stacked_lstm_cell, _X, initial_state=_istate)\n",
    "\n",
    "    # Linear activation\n",
    "    # Get inner loop last output\n",
    "    return tf.matmul(outputs[-1], _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = RNN(x, istate, weights, biases)\n",
    "\n",
    "#cost function \n",
    "cost = tf.reduce_mean(tf.pow(pred-y,2)) # cost function of this batch of data\n",
    "#cost2 = tf.abs(cost1)\n",
    "#compute parameter updates\n",
    "#train_op = tf.train.GradientDescentOptimizer(0.008).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.005, 0.3).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: generate validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data: x_val shape - (3360, 48, 1); y_val shape - (3360, 1)\n"
     ]
    }
   ],
   "source": [
    "x_val,y_val = test_data_gen(n_steps,batch_size)\n",
    "print \"test data: x_val shape - {}; y_val shape - {}\".format(x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 7: run rnn network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss ---- Train = 0.093655; Test = 0.177809\n",
      "Iter 1, Minibatch Loss ---- Train = 0.081665; Test = 0.174156\n",
      "Iter 2, Minibatch Loss ---- Train = 0.076619; Test = 0.172438\n",
      "Iter 3, Minibatch Loss ---- Train = 0.074930; Test = 0.172424\n",
      "Iter 4, Minibatch Loss ---- Train = 0.075649; Test = 0.171902\n",
      "Iter 5, Minibatch Loss ---- Train = 0.068811; Test = 0.167265\n",
      "Iter 6, Minibatch Loss ---- Train = 0.066549; Test = 0.165730\n",
      "Iter 7, Minibatch Loss ---- Train = 0.069917; Test = 0.159447\n",
      "Iter 8, Minibatch Loss ---- Train = 0.060479; Test = 0.160214\n",
      "Iter 9, Minibatch Loss ---- Train = 0.060076; Test = 0.153681\n",
      "Iter 10, Minibatch Loss ---- Train = 0.054533; Test = 0.153607\n",
      "Iter 11, Minibatch Loss ---- Train = 0.052774; Test = 0.148411\n",
      "Iter 12, Minibatch Loss ---- Train = 0.048966; Test = 0.147337\n",
      "Iter 13, Minibatch Loss ---- Train = 0.047992; Test = 0.143373\n",
      "Iter 14, Minibatch Loss ---- Train = 0.046384; Test = 0.143013\n",
      "Iter 15, Minibatch Loss ---- Train = 0.044880; Test = 0.139634\n",
      "Iter 16, Minibatch Loss ---- Train = 0.041327; Test = 0.137878\n",
      "Iter 17, Minibatch Loss ---- Train = 0.042936; Test = 0.135690\n",
      "Iter 18, Minibatch Loss ---- Train = 0.041616; Test = 0.134314\n",
      "Iter 19, Minibatch Loss ---- Train = 0.044030; Test = 0.132196\n",
      "Iter 20, Minibatch Loss ---- Train = 0.037318; Test = 0.130796\n",
      "Iter 21, Minibatch Loss ---- Train = 0.039418; Test = 0.130043\n",
      "Iter 22, Minibatch Loss ---- Train = 0.039560; Test = 0.128279\n",
      "Iter 23, Minibatch Loss ---- Train = 0.038846; Test = 0.127496\n",
      "Iter 24, Minibatch Loss ---- Train = 0.037630; Test = 0.125962\n",
      "Iter 25, Minibatch Loss ---- Train = 0.037010; Test = 0.124526\n",
      "Iter 26, Minibatch Loss ---- Train = 0.033057; Test = 0.123251\n",
      "Iter 27, Minibatch Loss ---- Train = 0.032763; Test = 0.123792\n",
      "Iter 28, Minibatch Loss ---- Train = 0.035942; Test = 0.121567\n",
      "Iter 29, Minibatch Loss ---- Train = 0.032543; Test = 0.120944\n",
      "Iter 30, Minibatch Loss ---- Train = 0.032760; Test = 0.119415\n",
      "Iter 31, Minibatch Loss ---- Train = 0.034764; Test = 0.118990\n",
      "Iter 32, Minibatch Loss ---- Train = 0.032324; Test = 0.117550\n",
      "Iter 33, Minibatch Loss ---- Train = 0.035031; Test = 0.117131\n",
      "Iter 34, Minibatch Loss ---- Train = 0.035957; Test = 0.115553\n",
      "Iter 35, Minibatch Loss ---- Train = 0.032302; Test = 0.114824\n",
      "Iter 36, Minibatch Loss ---- Train = 0.032517; Test = 0.113713\n",
      "Iter 37, Minibatch Loss ---- Train = 0.028849; Test = 0.113262\n",
      "Iter 38, Minibatch Loss ---- Train = 0.031598; Test = 0.111945\n",
      "Iter 39, Minibatch Loss ---- Train = 0.028223; Test = 0.111684\n",
      "Iter 40, Minibatch Loss ---- Train = 0.026192; Test = 0.109947\n",
      "Iter 41, Minibatch Loss ---- Train = 0.031537; Test = 0.110199\n",
      "Iter 42, Minibatch Loss ---- Train = 0.027758; Test = 0.108916\n",
      "Iter 43, Minibatch Loss ---- Train = 0.027706; Test = 0.108694\n",
      "Iter 44, Minibatch Loss ---- Train = 0.026585; Test = 0.107557\n",
      "Iter 45, Minibatch Loss ---- Train = 0.030849; Test = 0.107413\n",
      "Iter 46, Minibatch Loss ---- Train = 0.024476; Test = 0.106079\n",
      "Iter 47, Minibatch Loss ---- Train = 0.028161; Test = 0.106543\n",
      "Iter 48, Minibatch Loss ---- Train = 0.028373; Test = 0.105416\n",
      "Iter 49, Minibatch Loss ---- Train = 0.030907; Test = 0.105250\n",
      "Iter 50, Minibatch Loss ---- Train = 0.025813; Test = 0.103770\n",
      "Iter 51, Minibatch Loss ---- Train = 0.029191; Test = 0.103942\n",
      "Iter 52, Minibatch Loss ---- Train = 0.027031; Test = 0.103573\n",
      "Iter 53, Minibatch Loss ---- Train = 0.025406; Test = 0.103445\n",
      "Iter 54, Minibatch Loss ---- Train = 0.026098; Test = 0.102158\n",
      "Iter 55, Minibatch Loss ---- Train = 0.024206; Test = 0.102599\n",
      "Iter 56, Minibatch Loss ---- Train = 0.026117; Test = 0.101766\n",
      "Iter 57, Minibatch Loss ---- Train = 0.028887; Test = 0.102443\n",
      "Iter 58, Minibatch Loss ---- Train = 0.025963; Test = 0.100916\n",
      "Iter 59, Minibatch Loss ---- Train = 0.026311; Test = 0.101264\n",
      "Iter 60, Minibatch Loss ---- Train = 0.027211; Test = 0.100310\n",
      "Iter 61, Minibatch Loss ---- Train = 0.027563; Test = 0.100396\n",
      "Iter 62, Minibatch Loss ---- Train = 0.025463; Test = 0.099047\n",
      "Iter 63, Minibatch Loss ---- Train = 0.028626; Test = 0.099897\n",
      "Iter 64, Minibatch Loss ---- Train = 0.027490; Test = 0.098266\n",
      "Iter 65, Minibatch Loss ---- Train = 0.024252; Test = 0.098482\n",
      "Iter 66, Minibatch Loss ---- Train = 0.026300; Test = 0.097647\n",
      "Iter 67, Minibatch Loss ---- Train = 0.026248; Test = 0.098216\n",
      "Iter 68, Minibatch Loss ---- Train = 0.026749; Test = 0.096716\n",
      "Iter 69, Minibatch Loss ---- Train = 0.024115; Test = 0.097767\n",
      "Iter 70, Minibatch Loss ---- Train = 0.024245; Test = 0.096161\n",
      "Iter 71, Minibatch Loss ---- Train = 0.026878; Test = 0.097396\n",
      "Iter 72, Minibatch Loss ---- Train = 0.027491; Test = 0.095890\n",
      "Iter 73, Minibatch Loss ---- Train = 0.023483; Test = 0.096441\n",
      "Iter 74, Minibatch Loss ---- Train = 0.024730; Test = 0.094851\n",
      "Iter 75, Minibatch Loss ---- Train = 0.025452; Test = 0.095826\n",
      "Iter 76, Minibatch Loss ---- Train = 0.024909; Test = 0.094534\n",
      "Iter 77, Minibatch Loss ---- Train = 0.023807; Test = 0.095478\n",
      "Iter 78, Minibatch Loss ---- Train = 0.024497; Test = 0.093861\n",
      "Iter 79, Minibatch Loss ---- Train = 0.025851; Test = 0.094819\n",
      "Iter 80, Minibatch Loss ---- Train = 0.024291; Test = 0.093303\n",
      "Iter 81, Minibatch Loss ---- Train = 0.025019; Test = 0.094808\n",
      "Iter 82, Minibatch Loss ---- Train = 0.022798; Test = 0.092819\n",
      "Iter 83, Minibatch Loss ---- Train = 0.026559; Test = 0.093514\n",
      "Iter 84, Minibatch Loss ---- Train = 0.023303; Test = 0.092518\n",
      "Iter 85, Minibatch Loss ---- Train = 0.025495; Test = 0.093100\n",
      "Iter 86, Minibatch Loss ---- Train = 0.021662; Test = 0.091735\n",
      "Iter 87, Minibatch Loss ---- Train = 0.029874; Test = 0.092585\n",
      "Iter 88, Minibatch Loss ---- Train = 0.023591; Test = 0.091432\n",
      "Iter 89, Minibatch Loss ---- Train = 0.024930; Test = 0.092616\n",
      "Iter 90, Minibatch Loss ---- Train = 0.023725; Test = 0.090704\n",
      "Iter 91, Minibatch Loss ---- Train = 0.024556; Test = 0.091388\n",
      "Iter 92, Minibatch Loss ---- Train = 0.026238; Test = 0.090396\n",
      "Iter 93, Minibatch Loss ---- Train = 0.021121; Test = 0.091346\n",
      "Iter 94, Minibatch Loss ---- Train = 0.023554; Test = 0.089817\n",
      "Iter 95, Minibatch Loss ---- Train = 0.021838; Test = 0.091000\n",
      "Iter 96, Minibatch Loss ---- Train = 0.023360; Test = 0.089473\n",
      "Iter 97, Minibatch Loss ---- Train = 0.024933; Test = 0.090676\n",
      "Iter 98, Minibatch Loss ---- Train = 0.024417; Test = 0.089316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/pymodules/python2.7/matplotlib/axes.py:4747: UserWarning: No labeled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labeled objects found. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: [[ 1.          0.66656463]\n",
      " [ 0.66656463  1.        ]], R-square: [[ 1.          0.44430841]\n",
      " [ 0.44430841  1.        ]]\n",
      "Iter 99, Minibatch Loss ---- Train = 0.022988; Test = 0.090434\n"
     ]
    }
   ],
   "source": [
    "### Execute\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "outp = []\n",
    "with tf.Session() as sess:\n",
    "    # Create a summary to monitor cost function\n",
    "    tf.scalar_summary(\"loss\", cost)\n",
    "    #tf.scalar_summary(\"loss2\",cost2)\n",
    "    # Merge all summaries to a single operator\n",
    "    merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # tensorboard info.# Set logs writer into folder /tmp/tensorflow_logs\n",
    "    summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def)\n",
    "    \n",
    "    #initialize all variables in the model\n",
    "    sess.run(init)\n",
    "    for k in range(num_epoches):\n",
    "        #Generate Data for each epoch\n",
    "        #What this does is it creates a list of of elements of length seq_len, each of size [batch_size,input_size]\n",
    "        #this is required to feed data into rnn.rnn\n",
    "        X,Y = train_data_gen(n_steps,batch_size)\n",
    "        X = X.reshape(batch_size,n_steps,feature_size)\n",
    "        \n",
    "        \n",
    "        #Create the dictionary of inputs to feed into sess.run\n",
    "        sess.run(optimizer,feed_dict={x:X,y:Y,istate:np.zeros((batch_size,num_layers*2*n_hidden))})   \n",
    "        #perform an update on the parameters\n",
    "        \n",
    "        loss1 = sess.run(cost, feed_dict = {x:X,y:Y,istate:np.zeros((batch_size,num_layers*2*n_hidden))} )\n",
    "        loss2 = sess.run(cost, feed_dict = {x:x_val,y:y_val,istate:np.zeros((batch_size,num_layers*2*n_hidden))} )            #compute the cost on the validation set\n",
    "        output_tmp = sess.run(pred,feed_dict = {x:X,y:Y,istate:np.zeros((batch_size,num_layers*2*n_hidden))} )\n",
    "        outp_train = output_tmp\n",
    "        output_tmp = sess.run(pred,feed_dict = {x:x_val,y:y_val,istate:np.zeros((batch_size,num_layers*2*n_hidden))} )\n",
    "        outp_test = output_tmp\n",
    "        \n",
    "        if k == num_epoches-1:\n",
    "            outp_train = np.array(outp_train)\n",
    "            xxx = np.arange(0,test_len)\n",
    "            pl.plot(xxx,outp_train,color = \"red\")\n",
    "            pl.plot(xxx,Y)\n",
    "            pl.grid()\n",
    "            pl.legend()\n",
    "            pl.show()\n",
    "            R = np.corrcoef(outp_train.T,Y.T)\n",
    "            RR = R**2\n",
    "            print \"R: {}, R-square: {}\".format(R,RR)\n",
    "            \n",
    "        # Write logs at every iteration\n",
    "        summary_str = sess.run(merged_summary_op, feed_dict={x:x_val,y:y_val,istate:np.zeros((batch_size,num_layers*2*n_hidden))} )\n",
    "        summary_writer.add_summary(summary_str, k)\n",
    "        print \"Iter \" + str(k) + \", Minibatch Loss ---- Train = \" + \"{:.6f}\".format(loss1) + \"; Test = \" + \"{:.6f}\".format(loss2)\n",
    "    #print \"haha{}\".format(outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3360, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.array(outp_test)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3360, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3360, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.dtype = float\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = np.corrcoef(out.T,y_val.T)\n",
    "RR = R**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.58340399],\n",
       "       [ 0.58340399,  1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final R\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.34036022],\n",
       "       [ 0.34036022,  1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final R-square\n",
    "RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xxx = np.arange(0,test_len)\n",
    "pl.plot(xxx,out,color = \"red\")\n",
    "pl.plot(xxx,y_val)\n",
    "pl.grid()\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09043369367934595"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final MSE\n",
    "sq = pow(out-y_val,2)\n",
    "np.mean(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.7019429207\n"
     ]
    }
   ],
   "source": [
    "# run time\n",
    "time2 = time.time()\n",
    "print time2-time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function subplot in module matplotlib.pyplot:\n",
      "\n",
      "subplot(*args, **kwargs)\n",
      "    Return a subplot axes positioned by the given grid definition.\n",
      "    \n",
      "    Typical call signature::\n",
      "    \n",
      "      subplot(nrows, ncols, plot_number)\n",
      "    \n",
      "    Where *nrows* and *ncols* are used to notionally split the figure\n",
      "    into ``nrows * ncols`` sub-axes, and *plot_number* is used to identify\n",
      "    the particular subplot that this function is to create within the notional\n",
      "    grid. *plot_number* starts at 1, increments across rows first and has a\n",
      "    maximum of ``nrows * ncols``.\n",
      "    \n",
      "    In the case when *nrows*, *ncols* and *plot_number* are all less than 10,\n",
      "    a convenience exists, such that the a 3 digit number can be given instead,\n",
      "    where the hundreds represent *nrows*, the tens represent *ncols* and the\n",
      "    units represent *plot_number*. For instance::\n",
      "    \n",
      "      subplot(211)\n",
      "    \n",
      "    produces a subaxes in a figure which represents the top plot (i.e. the\n",
      "    first) in a 2 row by 1 column notional grid (no grid actually exists,\n",
      "    but conceptually this is how the returned subplot has been positioned).\n",
      "    \n",
      "    .. note::\n",
      "    \n",
      "       Creating a new subplot with a position which is entirely inside a\n",
      "       pre-existing axes will trigger the larger axes to be deleted::\n",
      "    \n",
      "          import matplotlib.pyplot as plt\n",
      "          # plot a line, implicitly creating a subplot(111)\n",
      "          plt.plot([1,2,3])\n",
      "          # now create a subplot which represents the top plot of a grid\n",
      "          # with 2 rows and 1 column. Since this subplot will overlap the\n",
      "          # first, the plot (and its axes) previously created, will be removed\n",
      "          plt.subplot(211)\n",
      "          plt.plot(range(12))\n",
      "          plt.subplot(212, axisbg='y') # creates 2nd subplot with yellow background\n",
      "    \n",
      "       If you do not want this behavior, use the\n",
      "       :meth:`~matplotlib.figure.Figure.add_subplot` method or the\n",
      "       :func:`~matplotlib.pyplot.axes` function instead.\n",
      "    \n",
      "    Keyword arguments:\n",
      "    \n",
      "      *axisbg*:\n",
      "        The background color of the subplot, which can be any valid\n",
      "        color specifier.  See :mod:`matplotlib.colors` for more\n",
      "        information.\n",
      "    \n",
      "      *polar*:\n",
      "        A boolean flag indicating whether the subplot plot should be\n",
      "        a polar projection.  Defaults to *False*.\n",
      "    \n",
      "      *projection*:\n",
      "        A string giving the name of a custom projection to be used\n",
      "        for the subplot. This projection must have been previously\n",
      "        registered. See :mod:`matplotlib.projections`.\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "        :func:`~matplotlib.pyplot.axes`\n",
      "            For additional information on :func:`axes` and\n",
      "            :func:`subplot` keyword arguments.\n",
      "    \n",
      "        :file:`examples/pie_and_polar_charts/polar_scatter_demo.py`\n",
      "            For an example\n",
      "    \n",
      "    **Example:**\n",
      "    \n",
      "    .. plot:: mpl_examples/subplots_axes_and_figures/subplot_demo.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pl.subplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14 ],\n",
       "       [ 0.111],\n",
       "       [ 0.072],\n",
       "       ..., \n",
       "       [ 0.113],\n",
       "       [ 0.446],\n",
       "       [ 0.113]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
