{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning (RNN) Demo for Load Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import all the packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn, rnn_cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: setting all global parameters -- sec 2 network configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time1 = time.time() # set up counter to record run time\n",
    "data_dir = './data/' # directory contains input data\n",
    "num_epoches = 1500 # training epoches for each customer samples\n",
    "n_steps = 48 # input size\n",
    "test_batch_size = 70*48 # days of a batch\n",
    "train_batch_size = 2*48\n",
    "feature_size = 1 # same time of a week\n",
    "n_hidden = 20 # input size\n",
    "num_layers = 5\n",
    "n_output = 1\n",
    "Rs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 4: define data generating function code. \n",
    "which generate a batch of batch-size large sequence data. the data is feature_size dims width and is a time series of float32 of steps steps. inputs and outputs are:\n",
    "\n",
    "inputs:\n",
    "----n_batch: number of samples in a batch\n",
    "----steps: the sequence length of a sample data\n",
    "----feature_size: dimensions of a single time step data frame\n",
    "\n",
    "outputs:\n",
    "----X inputs, shape(n_batch,steps,feature_size)\n",
    "----Y outputs should be, shape(n_batch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_gen(totaltraindays,x_data,y_data,steps = 48, n_batch = train_batch_size):\n",
    "    X = np.zeros((n_batch,steps,feature_size))\n",
    "    Y = np.zeros((n_batch,feature_size))\n",
    "    rang = range(totaltraindays) # test day sample range\n",
    "    train_days_list = rd.sample(rang,n_batch) # pick unduplicated n indexes as examples\n",
    "    #print totaltraindays\n",
    "    tmpX = [x_data[i,0-steps:] for i in train_days_list]\n",
    "    tmpY = [y_data[i,:] for i in train_days_list]\n",
    "    X = np.array(tmpX).reshape(n_batch,steps,feature_size)\n",
    "    Y = np.array(tmpY).reshape(n_batch,feature_size)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_data_gen(x_data,y_data,steps = 48, n_batch = test_batch_size):\n",
    "    X = np.zeros((n_batch,steps,feature_size))\n",
    "    Y = np.zeros((n_batch,feature_size))\n",
    "    #print x_data[:,0-steps:].shape,y_data.shape\n",
    "    #print n_batch, steps\n",
    "    X = x_data[:,0-steps:].reshape(n_batch,steps,feature_size)\n",
    "    Y = y_data.reshape(n_batch,feature_size)\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: construct RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create placeholder for x and y\n",
    "x = tf.placeholder(\"float\",[None,n_steps,feature_size])\n",
    "istate = tf.placeholder(\"float\",[None,num_layers*2*n_hidden])\n",
    "y = tf.placeholder(\"float\",[None,n_output])\n",
    "\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([feature_size, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_output]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_output]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(_X, _istate, _weights, _biases):\n",
    "\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, feature_size]) # (n_steps*batch_size, n_input)\n",
    "    # Linear activation\n",
    "    _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden']\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    stacked_lstm_cell = rnn_cell.MultiRNNCell([lstm_cell]*num_layers)\n",
    "    \n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(stacked_lstm_cell, _X, initial_state=_istate)\n",
    "\n",
    "    # Linear activation\n",
    "    # Get inner loop last output\n",
    "    return tf.matmul(outputs[-1], _weights['out']) + _biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = RNN(x, istate, weights, biases)\n",
    "\n",
    "#cost function \n",
    "cost = tf.reduce_mean(tf.pow(pred-y,2)) # cost function of this batch of data\n",
    "#compute parameter updates\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.2).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optimizer2 = tf.train.RMSPropOptimizer(0.005, 0.3).minimize(cost2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n",
      "Iter 1481\n",
      "Iter 1482\n",
      "Iter 1483\n",
      "Iter 1484\n",
      "Iter 1485\n",
      "Iter 1486\n",
      "Iter 1487\n",
      "Iter 1488\n",
      "Iter 1489\n",
      "Iter 1490\n",
      "Iter 1491\n",
      "Iter 1492\n",
      "Iter 1493\n",
      "Iter 1494\n",
      "Iter 1495\n",
      "Iter 1496\n",
      "Iter 1497\n",
      "Iter 1498\n",
      "Iter 1499\n"
     ]
    }
   ],
   "source": [
    "## iterating among all customers to find current training customer\n",
    "#cus_list = [4,5,8,9]\n",
    "result_final = []\n",
    "for i in range(10,70):\n",
    "    accuracy = []\n",
    "    accuracy1 = []\n",
    "    ii = i#cus_list[i]\n",
    "    test_x_name = data_dir + 'test_x_' + str(ii) + '.csv'\n",
    "    test_y_name = data_dir + 'test_y_' + str(ii) + '.csv'\n",
    "    train_x_name = data_dir + 'train_x_' + str(ii) + '.csv'\n",
    "    train_y_name = data_dir + 'train_y_' + str(ii) + '.csv'\n",
    "    tmp_data = np.array(pd.read_csv(test_x_name,header = None))\n",
    "    test_x_data = tmp_data[:,1:]\n",
    "    # print test_x_data.dtype  data are stored as float64 double precision format\n",
    "    tmp_data = np.array(pd.read_csv(test_y_name,header = None))\n",
    "    test_y_data = tmp_data[:,1:]\n",
    "    tmp_data = np.array(pd.read_csv(train_x_name,header = None))\n",
    "    train_x_data = tmp_data[:,1:]\n",
    "    tmp_data = np.array(pd.read_csv(train_y_name,header = None))\n",
    "    train_y_data = tmp_data[:,1:]\n",
    "    #log them\n",
    "    #test_x_data = np.log(test_x_data)\n",
    "    #test_y_data = np.log(test_y_data)\n",
    "    #train_x_data = np.log(train_x_data)\n",
    "    #train_y_data = np.log(train_y_data)\n",
    "    \n",
    "    traindays = train_y_data.shape[0]\n",
    "    # generate test data\n",
    "    test_x,test_y = test_data_gen(test_x_data,test_y_data,n_steps)\n",
    "    test_x = test_x.reshape(test_batch_size,n_steps,feature_size)\n",
    "    ### Execute\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    outp = []\n",
    "    outlist = np.zeros([Rs,test_batch_size])\n",
    "    with tf.Session() as sess:\n",
    "        # Create a summary to monitor cost function\n",
    "        #tf.scalar_summary(\"loss\", cost)\n",
    "        # Merge all summaries to a single operator\n",
    "        #merged_summary_op = tf.merge_all_summaries()\n",
    "\n",
    "        # tensorboard info.# Set logs writer into folder /tmp/tensorflow_logs\n",
    "        #summary_writer = tf.train.SummaryWriter('/tmp/tensorflow_logs', graph_def=sess.graph_def)\n",
    "\n",
    "        #initialize all variables in the model\n",
    "        sess.run(init)\n",
    "        for k in range(num_epoches):\n",
    "            #Generate Data for each epoch\n",
    "            #What this does is it creates a list of of elements of length seq_len, each of size [batch_size,input_size]\n",
    "            #this is required to feed data into rnn.rnn\n",
    "            #print traindays\n",
    "            X,Y = train_data_gen(traindays,train_x_data,train_y_data,n_steps)\n",
    "            X = X.reshape(train_batch_size,n_steps,feature_size)\n",
    "\n",
    "\n",
    "            #Create the dictionary of inputs to feed into sess.run\n",
    "            #if k < 0:\n",
    "            #    sess.run(optimizer2,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})\n",
    "            #else:\n",
    "            sess.run(optimizer,feed_dict={x:X,y:Y,istate:np.zeros((train_batch_size,num_layers*2*n_hidden))})   \n",
    "            #perform an update on the parameters\n",
    "\n",
    "            # Write logs at every iteration\n",
    "            #if k>50 & k%10 == 0:\n",
    "            #    summary_str = sess.run(merged_summary_op, feed_dict={x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "            #    summary_writer.add_summary(summary_str, k)\n",
    "            \n",
    "            #if k % 10 == 0:\n",
    "            if k > num_epoches-Rs:\n",
    "                output_tmp_ex = sess.run(pred,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "                print \"Iter \" + str(k)# + \", Minibatch Loss ---- Train = \" + \"{:.6f}\".format(loss1) + \"; Test = \" + \"{:.6f}\".format(loss2)\n",
    "        #print \"haha{}\".format(outp)\n",
    "                ktmp = np.corrcoef(output_tmp_ex.T,test_y.T)[0,1]\n",
    "                accuracy.append(ktmp)\n",
    "            #    print ktmp\n",
    "            #if k % 10 == 0:\n",
    "            #    output_tmp_ex = sess.run(pred,feed_dict = {x:test_x,y:test_y,istate:np.zeros((test_batch_size,num_layers*2*n_hidden))} )\n",
    "            #    print \"Iter \" + str(k)# + \", Minibatch Loss ---- Train = \" + \"{:.6f}\".format(loss1) + \"; Test = \" + \"{:.6f}\".format(loss2)\n",
    "        #print \"haha{}\".format(outp)\n",
    "            #    ktmp = np.corrcoef(output_tmp_ex.T,test_y.T)[0,1]\n",
    "            #    accuracy1.append(ktmp)\n",
    "            #    print ktmp\n",
    "    outname = './single_result/R-house' + str(i) + 'size-5-20-single.csv'\n",
    "    #outname1 = 'accu-house' + str(i) + 'size-5-30-single.csv'\n",
    "    DataFrame(accuracy).to_csv(outname)\n",
    "    #DataFrame(accuracy1).to_csv(outname1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Draw\n",
    "#xxx = np.arange(0,test_batch_size)\n",
    "#pl.plot(xxx,out,color = \"red\")\n",
    "#pl.plot(xxx,test_y)\n",
    "#pl.grid()\n",
    "##pl.legend()\n",
    "#pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#out = np.array(outlist[0])\n",
    "#np.corrcoef(out.T,test_y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DataFrame(result_final).to_csv('result2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a = np.array(a).reshape((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#b = np.log(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#c = np.exp(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdamOptimizer in module tensorflow.python.training.adam:\n",
      "\n",
      "class AdamOptimizer(tensorflow.python.training.optimizer.Optimizer)\n",
      " |  Optimizer that implements the Adam algorithm.\n",
      " |  \n",
      " |  See [Kingma et. al., 2014](http://arxiv.org/abs/1412.6980)\n",
      " |  ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n",
      " |  \n",
      " |  @@__init__\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdamOptimizer\n",
      " |      tensorflow.python.training.optimizer.Optimizer\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')\n",
      " |      Construct a new Adam optimizer.\n",
      " |      \n",
      " |      Initialization:\n",
      " |      \n",
      " |      ```\n",
      " |      m_0 <- 0 (Initialize initial 1st moment vector)\n",
      " |      v_0 <- 0 (Initialize initial 2nd moment vector)\n",
      " |      t <- 0 (Initialize timestep)\n",
      " |      ```\n",
      " |      \n",
      " |      The update rule for `variable` with gradient `g` uses an optimization\n",
      " |      described at the end of section2 of the paper:\n",
      " |      \n",
      " |      ```\n",
      " |      t <- t + 1\n",
      " |      lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
      " |      \n",
      " |      m_t <- beta1 * m_{t-1} + (1 - beta1) * g\n",
      " |      v_t <- beta2 * v_{t-1} + (1 - beta2) * g * g\n",
      " |      variable <- variable - lr_t * m_t / (sqrt(v_t) + epsilon)\n",
      " |      ```\n",
      " |      \n",
      " |      The default value of 1e-8 for epsilon might not be a good default in\n",
      " |      general. For example, when training an Inception network on ImageNet a\n",
      " |      current good choice is 1.0 or 0.1.\n",
      " |      \n",
      " |      Args:\n",
      " |        learning_rate: A Tensor or a floating point value.  The learning rate.\n",
      " |        beta1: A float value or a constant float tensor.\n",
      " |          The exponential decay rate for the 1st moment estimates.\n",
      " |        beta2: A float value or a constant float tensor.\n",
      " |          The exponential decay rate for the 2nd moment estimates.\n",
      " |        epsilon: A small constant for numerical stability.\n",
      " |        use_locking: If True use locks for update operations.\n",
      " |        name: Optional name for the operations created when applying gradients.\n",
      " |          Defaults to \"Adam\".\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  apply_gradients(self, grads_and_vars, global_step=None, name=None)\n",
      " |      Apply gradients to variables.\n",
      " |      \n",
      " |      This is the second part of `minimize()`. It returns an `Operation` that\n",
      " |      applies gradients.\n",
      " |      \n",
      " |      Args:\n",
      " |        grads_and_vars: List of (gradient, variable) pairs as returned by\n",
      " |          `compute_gradients()`.\n",
      " |        global_step: Optional `Variable` to increment by one after the\n",
      " |          variables have been updated.\n",
      " |        name: Optional name for the returned operation.  Default to the\n",
      " |          name passed to the `Optimizer` constructor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` that applies the specified gradients. If `global_step`\n",
      " |        was not None, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `grads_and_vars` is malformed.\n",
      " |        ValueError: If none of the variables have gradients.\n",
      " |  \n",
      " |  compute_gradients(self, loss, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False)\n",
      " |      Compute gradients of `loss` for the variables in `var_list`.\n",
      " |      \n",
      " |      This is the first part of `minimize()`.  It returns a list\n",
      " |      of (gradient, variable) pairs where \"gradient\" is the gradient\n",
      " |      for \"variable\".  Note that \"gradient\" can be a `Tensor`, an\n",
      " |      `IndexedSlices`, or `None` if there is no gradient for the\n",
      " |      given variable.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A Tensor containing the value to minimize.\n",
      " |        var_list: Optional list of tf.Variable to update to minimize\n",
      " |          `loss`.  Defaults to the list of variables collected in the graph\n",
      " |          under the key `GraphKey.TRAINABLE_VARIABLES`.\n",
      " |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      " |          `GATE_NONE`, `GATE_OP`, or `GATE_GRAPH`.\n",
      " |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      " |          Valid values are defined in the class `AggregationMethod`.\n",
      " |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      " |          the corresponding op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of (gradient, variable) pairs.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `var_list` contains anything else than `Variable` objects.\n",
      " |        ValueError: If some arguments are invalid.\n",
      " |  \n",
      " |  get_slot(self, var, name)\n",
      " |      Return a slot named `name` created for `var` by the Optimizer.\n",
      " |      \n",
      " |      Some `Optimizer` subclasses use additional variables.  For example\n",
      " |      `Momentum` and `Adagrad` use variables to accumulate updates.  This method\n",
      " |      gives access to these `Variable` objects if for some reason you need them.\n",
      " |      \n",
      " |      Use `get_slot_names()` to get the list of slot names created by the\n",
      " |      `Optimizer`.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: A variable passed to `minimize()` or `apply_gradients()`.\n",
      " |        name: A string.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Variable` for the slot if it was created, `None` otherwise.\n",
      " |  \n",
      " |  get_slot_names(self)\n",
      " |      Return a list of the names of slots created by the `Optimizer`.\n",
      " |      \n",
      " |      See `get_slot()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of strings.\n",
      " |  \n",
      " |  minimize(self, loss, global_step=None, var_list=None, gate_gradients=1, aggregation_method=None, colocate_gradients_with_ops=False, name=None)\n",
      " |      Add operations to minimize `loss` by updating `var_list`.\n",
      " |      \n",
      " |      This method simply combines calls `compute_gradients()` and\n",
      " |      `apply_gradients()`. If you want to process the gradient before applying\n",
      " |      them call `compute_gradients()` and `apply_gradients()` explicitly instead\n",
      " |      of using this function.\n",
      " |      \n",
      " |      Args:\n",
      " |        loss: A `Tensor` containing the value to minimize.\n",
      " |        global_step: Optional `Variable` to increment by one after the\n",
      " |          variables have been updated.\n",
      " |        var_list: Optional list of `Variable` objects to update to minimize\n",
      " |          `loss`.  Defaults to the list of variables collected in the graph\n",
      " |          under the key `GraphKeys.TRAINABLE_VARIABLES`.\n",
      " |        gate_gradients: How to gate the computation of gradients.  Can be\n",
      " |          `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n",
      " |        aggregation_method: Specifies the method used to combine gradient terms.\n",
      " |          Valid values are defined in the class `AggregationMethod`.\n",
      " |        colocate_gradients_with_ops: If True, try colocating gradients with\n",
      " |          the corresponding op.\n",
      " |        name: Optional name for the returned operation.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An Operation that updates the variables in `var_list`.  If `global_step`\n",
      " |        was not `None`, that operation also increments `global_step`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If some of the variables are not `Variable` objects.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tensorflow.python.training.optimizer.Optimizer:\n",
      " |  \n",
      " |  GATE_GRAPH = 2\n",
      " |  \n",
      " |  GATE_NONE = 0\n",
      " |  \n",
      " |  GATE_OP = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
